# Home Assistant Voice Assistant Stack
# Funktioniert mit Docker Compose und Podman Compose
#
# Starten: docker compose up -d
# Oder:    podman-compose up -d

services:
  # ============== HOME ASSISTANT ==============
  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: homeassistant
    restart: unless-stopped
    privileged: true
    ports:
      - "8123:8123"
    volumes:
      - ./homeassistant/config:/config
      - /etc/localtime:/etc/localtime:ro
    environment:
      - TZ=Europe/Berlin
    depends_on:
      - whisper
      - piper
      - ollama
    networks:
      - voice-net

  # ============== WHISPER (Speech-to-Text) ==============
  whisper:
    image: rhasspy/wyoming-whisper:latest
    container_name: whisper
    restart: unless-stopped
    ports:
      - "10300:10300"
    volumes:
      - ./whisper/data:/data
    command: >
      --model base
      --language de
      --data-dir /data
      --download-dir /data
    environment:
      - TZ=Europe/Berlin
    networks:
      - voice-net

  # ============== PIPER (Text-to-Speech) ==============
  piper:
    image: rhasspy/wyoming-piper:latest
    container_name: piper
    restart: unless-stopped
    ports:
      - "10200:10200"
    volumes:
      - ./piper/data:/data
    command: >
      --voice de_DE-thorsten-high
      --data-dir /data
      --download-dir /data
    environment:
      - TZ=Europe/Berlin
    networks:
      - voice-net

  # ============== OLLAMA (Local LLM) ==============
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ./ollama/data:/root/.ollama
    devices:
      - nvidia.com/gpu=all  # CDI-basierter GPU-Zugriff für Podman
    environment:
      - TZ=Europe/Berlin
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_GPU=999              # Alle Layer auf GPU laden (wenn möglich)
      - OLLAMA_GPU_MEMORY_FRACTION=0.9  # 90% des GPU-Speichers nutzen
      - OLLAMA_CONTEXT_LENGTH=4096      # Reduzierte Context-Länge spart VRAM
    networks:
      - voice-net

  # ============== DOZZLE (Container Logs UI) ==============
  dozzle:
    image: amir20/dozzle:latest
    container_name: dozzle
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - /run/podman/podman.sock:/var/run/docker.sock:ro  # Podman-Socket auf Docker-Pfad mappen
    environment:
      - TZ=Europe/Berlin
    networks:
      - voice-net

networks:
  voice-net:
    driver: bridge
